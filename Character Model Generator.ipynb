{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/compat/compat.py:175: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imutils import paths\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# character table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "number = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "alphbet = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T',\n",
    "                 'U', 'V', 'W', 'X', 'Y', 'Z']\n",
    "chinese = ['zh_cuan', 'zh_e', 'zh_gan', 'zh_gan1', 'zh_gui', 'zh_gui1', 'zh_hei', 'zh_hu', 'zh_ji', 'zh_jin',\n",
    "                'zh_jing', 'zh_jl', 'zh_liao', 'zh_lu', 'zh_meng', 'zh_min', 'zh_ning', 'zh_qing', 'zh_qiong',\n",
    "                'zh_shan', 'zh_su', 'zh_sx', 'zh_wan', 'zh_xiang', 'zh_xin', 'zh_yu', 'zh_yu1', 'zh_yue', 'zh_yun',\n",
    "                'zh_zang', 'zh_zhe']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions (made in a same class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class char_recognition_cnn_net:\n",
    "    \n",
    "    # initial some useful variables\n",
    "    def __init__(self):\n",
    "        self.dataset = number + alphbet + chinese\n",
    "        self.dataset_length = len(self.dataset)\n",
    "        self.y_len = len(self.dataset)\n",
    "        self.batch_size = 100\n",
    "        self.image_size = 20\n",
    "        self.x_place = tf.placeholder(dtype=tf.float32, shape=[None, self.image_size, self.image_size], name='x_place')\n",
    "        self.y_place = tf.placeholder(dtype=tf.float32, shape=[None, self.y_len], name='y_place')\n",
    "        self.keep_place = tf.placeholder(dtype=tf.float32, name='keep_place')\n",
    "        \n",
    "    #The cnn architecture we used for this model training\n",
    "    def cnn_architecture(self):\n",
    "        #input layer - binary input only 1 channel\n",
    "        x_inputs = tf.reshape(self.x_place, shape=[-1, 20, 20, 1])\n",
    "        \n",
    "        #convelutional layer 1\n",
    "        conv_weight1 = tf.Variable(tf.random_normal(shape=[3, 3, 1, 32], stddev=0.01), dtype=tf.float32)\n",
    "        conv_bias1 = tf.Variable(tf.random_normal(shape=[32]), dtype=tf.float32)\n",
    "        conv_layer1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x_inputs,filter=conv_weight1,strides=[1,1,1,1],padding='SAME'),conv_bias1))\n",
    "        conv_layer1 = tf.nn.max_pool(conv_layer1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv_layer1 = tf.nn.dropout(conv_layer1, self.keep_place)\n",
    "        \n",
    "        #convelutional layer 2\n",
    "        conv_weight2 = tf.Variable(tf.random_normal(shape=[3, 3, 32, 64], stddev=0.01), dtype=tf.float32)\n",
    "        conv_bias2 = tf.Variable(tf.random_normal(shape=[64]), dtype=tf.float32)\n",
    "        conv_layer2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv_layer1,filter=conv_weight2,strides=[1,1,1,1],padding='SAME'),conv_bias2))\n",
    "        conv_layer2 = tf.nn.max_pool(conv_layer2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv_layer2 = tf.nn.dropout(conv_layer2, self.keep_place)\n",
    "        \n",
    "        #convelutional layer 3\n",
    "        conv_weight3 = tf.Variable(tf.random_normal(shape=[3, 3, 64, 128], stddev=0.01), dtype=tf.float32)\n",
    "        conv_bias3 = tf.Variable(tf.random_normal(shape=[128]), dtype=tf.float32)\n",
    "        conv_layer3 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv_layer2,filter=conv_weight3,strides=[1,1,1,1],padding='SAME'),conv_bias3))\n",
    "        conv_layer3 = tf.nn.max_pool(conv_layer3, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        conv_layer3 = tf.nn.dropout(conv_layer3, self.keep_place)\n",
    "        \n",
    "        #full convelutional layer\n",
    "        conv_layer_out = tf.reshape(conv_layer3, shape=[-1, 3 * 3 * 128])\n",
    "        \n",
    "        #fully connected layer 1\n",
    "        fully_con_weight1 = tf.Variable(tf.random_normal(shape=[3 * 3 * 128, 1024], stddev=0.01), dtype=tf.float32)\n",
    "        fully_con_bias1 = tf.Variable(tf.random_normal(shape=[1024]), dtype=tf.float32)\n",
    "        fully_con_layer1 = tf.nn.relu(tf.add(tf.matmul(conv_layer_out, fully_con_weight1), fully_con_bias1))\n",
    "        fully_con_layer1 = tf.nn.dropout(fully_con_layer1, self.keep_place)\n",
    "        \n",
    "        \n",
    "        #fully connected layer 2\n",
    "        fully_con_weight2 = tf.Variable(tf.random_normal(shape=[1024, 1024], stddev=0.01), dtype=tf.float32)\n",
    "        fully_con_bias2 = tf.Variable(tf.random_normal(shape=[1024]), dtype=tf.float32)\n",
    "        fully_con_layer2 = tf.nn.relu(tf.add(tf.matmul(fully_con_layer1, fully_con_weight2), fully_con_bias2))\n",
    "        fully_con_layer2 = tf.nn.dropout(fully_con_layer2, self.keep_place)\n",
    "\n",
    "        \n",
    "        #fully connected layer 3, output layer\n",
    "        fully_con_weight3 = tf.Variable(tf.random_normal(shape=[1024, self.dataset_length], stddev=0.01), dtype=tf.float32)\n",
    "        fully_con_bias3 = tf.Variable(tf.random_normal(shape=[self.dataset_length]), dtype=tf.float32)\n",
    "        fully_con_layer3 = tf.add(tf.matmul(fully_con_layer2, fully_con_weight3), fully_con_bias3, name='out_put')\n",
    "        \n",
    "        #output the last layer, output shape is class numbers\n",
    "        return fully_con_layer3\n",
    "    \n",
    "    \n",
    "    #initial data and labels x,y as two np.array\n",
    "    def init_data(self,dir):\n",
    "        X = []\n",
    "        y = []\n",
    "        #check the path exists or not\n",
    "        if not os.path.exists(data_dir):\n",
    "            raise ValueError('No such file folder')\n",
    "        files = list(paths.list_images(dir))\n",
    "        for file in files:\n",
    "            src_img = cv2.imread(file, cv2.COLOR_BGR2GRAY)\n",
    "            if src_img.ndim == 3:\n",
    "                continue\n",
    "            #resize the image as target size\n",
    "            resize_img = cv2.resize(src_img, (20, 20))\n",
    "            X.append(resize_img)\n",
    "            # get the file folder name for each class\n",
    "            dir_name = file.split(os.path.sep)[-2]\n",
    "            vector_y = [0 for i in range(len(self.dataset))]\n",
    "            index_y = self.dataset.index(dir_name)\n",
    "            vector_y[index_y] = 1\n",
    "            y.append(vector_y)\n",
    "        X = np.array(X)\n",
    "        y = np.array(y).reshape(-1, self.dataset_length)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self,data_dir,save_model_path):\n",
    "        #process and read x, y data and labels use the function\n",
    "        X, y = self.init_data(data_dir)\n",
    "        print('loaded' + str(len(y)) + 'dataset')\n",
    "        \n",
    "        #split data as train and validation data set for 8 to 2 ratio\n",
    "        train_x, test_x, train_y, test_y = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "        #read the architecture\n",
    "        out_puts = self.cnn_architecture()\n",
    "        predict = tf.nn.softmax(out_puts)\n",
    "        predict = tf.argmax(predict, axis=1)\n",
    "        actual_y_value = tf.argmax(self.y_place, axis=1)\n",
    "        accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, actual_y_value), dtype=tf.float32))\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=out_puts, labels=self.y_place))\n",
    "        #optimizer and learning rate\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "        train_step = opt.minimize(cost)\n",
    "        \n",
    "        #start the train session\n",
    "        sess = tf.Session()\n",
    "        init = tf.global_variables_initializer()\n",
    "        sess.run(init)\n",
    "        step = 0\n",
    "        savers = tf.train.Saver()\n",
    "        while True:\n",
    "            train_index = np.random.choice(len(train_x), self.batch_size, replace=False)\n",
    "            train_randx = train_x[train_index]\n",
    "            train_randy = train_y[train_index]\n",
    "            _, loss = sess.run([train_step, cost],\n",
    "                               feed_dict={self.x_place:train_randx,self.y_place:train_randy,self.keep_place:0.75})\n",
    "            step += 1\n",
    "\n",
    "            if step % 10 == 0:\n",
    "                test_index = np.random.choice(len(test_x), self.batch_size, replace=False)\n",
    "                test_randx = test_x[test_index]\n",
    "                test_randy = test_y[test_index]\n",
    "                acc = sess.run(accuracy,feed_dict={self.x_place : test_randx, self.y_place : test_randy,\n",
    "                                                   self.keep_place : 1.0})\n",
    "                print(step, loss)\n",
    "                # stop train both step >500 and accuracy > 99%\n",
    "                if step % 50 == 0:\n",
    "                    print('accuracy:' + str(acc))\n",
    "                if step % 500 == 0:\n",
    "                    savers.save(sess, save_model_path, global_step=step)\n",
    "                if acc > 0.99 and step > 500:\n",
    "                    #save model\n",
    "                    savers.save(sess, save_model_path, global_step=step)\n",
    "                    #close the train session\n",
    "                    sess.close()\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "    #initial data x as one np.array\n",
    "    def init_testData(self,dir):\n",
    "        test_X = []\n",
    "        #check the path exists or not\n",
    "        if not os.path.exists(test_dir):\n",
    "            raise ValueError('No such file folder')\n",
    "        files = list(paths.list_images(dir))\n",
    "        for file in files:\n",
    "            src_img = cv2.imread(file, cv2.COLOR_BGR2GRAY)\n",
    "            if src_img.ndim == 3:\n",
    "                continue\n",
    "            resize_img = cv2.resize(src_img, (20, 20))\n",
    "            test_X.append(resize_img)\n",
    "        test_X = np.array(test_X)\n",
    "        return test_X\n",
    "    \n",
    "\n",
    "    def test(self,x_images,model_path):\n",
    "        text_list = []\n",
    "        out_puts = self.cnn_architecture()\n",
    "        predict = tf.nn.softmax(out_puts)\n",
    "        predict = tf.argmax(predict, axis=1)\n",
    "        savers = tf.train.Saver()\n",
    "        #start session\n",
    "        sess = tf.Session()\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        savers.restore(sess, model_path)\n",
    "        preds = sess.run(predict, feed_dict={self.x_place: x_images, self.keep_place: 1.0})\n",
    "        for i in range(len(preds)):\n",
    "            pred = preds[i].astype(int)\n",
    "            text_list.append(self.dataset[pred])\n",
    "        #close session\n",
    "        sess.close()\n",
    "        return text_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-3-5b88372401f2>:24: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /Users/evan/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from /Users/evan/MS/Ece697/bishe /1/./Dataset/models/char/model.ckpt-510\n",
      "['zh_su', 'W', 'zh_meng', '8']\n"
     ]
    }
   ],
   "source": [
    "cur_pos = sys.path[0]\n",
    "data_dir = os.path.join(cur_pos, 'Dataset/char_train/')\n",
    "test_dir = os.path.join(cur_pos, 'Dataset/char_test')\n",
    "train_model_path = os.path.join(cur_pos, './Dataset/models/char/model.ckpt')\n",
    "model_path = os.path.join(cur_pos,'./Dataset/models/char/model.ckpt-510')\n",
    "\n",
    "train_flag = 0 #set flag = 1 to train, other numbers 0 etc. to test\n",
    "net = char_recognition_cnn_net()\n",
    "\n",
    "if train_flag == 1:\n",
    "    # train\n",
    "    net.train(data_dir,train_model_path)\n",
    "else:\n",
    "    # test\n",
    "    test_X = net.init_testData(test_dir)\n",
    "    text = net.test(test_X,model_path)\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
